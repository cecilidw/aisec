{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We will start up trying to implement an end-to-end process with similar algorithm and preprocessing as described in the original article (https://docs.microsoft.com/en-us/security/engineering/identifying-security-bug-reports) and see where that gets us first. \n",
        "\n",
        "If you are new to Jupyter Notebooks, find instructions on use here: https://jupyter-notebook.readthedocs.io/en/stable/ \n",
        "\n",
        "Short version: To run the contents of a cell, press ctrl+enter :)\n",
        "\n",
        "The first thing we need to do is connect to the Azure Machine Learning Workspace :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "ws=Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has been uploaded and registered in the workspace already, so we just need to get it from there"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#import dataset from ws:\n",
        "dataset = Dataset.get_by_name(ws, name='SecBugDataset')\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "# this is the dataset we want to use:\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the result from one of the labelers, found in the column L2. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# these are the bugs labeled as security bugs by labeler 2\n",
        "\n",
        "sb = df[df['L2'] == 'Integrity/Security']\n",
        "print(sb)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# how many bugs are labeled as security bugs? \n",
        "\n",
        "print(len(sb))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, lets run through locally what we want to deploy to run in the cloud:\n",
        "\n",
        "\n",
        "First of all, there are a couple of libraries we need to import. Internal AML libraries to work with datasets and to handle training runs that will be logged to our experiment, and libraries from the machine learning framework \"Scikit Learn\" which contains functionality for using a classifier algorithm to train on our dataset and output a model.  \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from azureml.core import Dataset, Run\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.externals import joblib"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could experiment with different labelers or even combine the results from the labelers as in the original article this dataset was retrieved from, but for now we will use the results from labeler 2, so we create a new column \"Label\", with the contents from the column with labels from labeler 2:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df['Label'] = [1 if x =='Integrity/Security' else 0 for x in df['L2']]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The field we want to use to predict if it is a security bug or not is\n",
        "the \"summary\" field. It is a short text, and the text must be translated into a \n",
        "representation the machine learning alogrithm understand. For this we use the tf-idf vectorization algorithm:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#do the vectorization - tf-idf\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=2)\n",
        "tfidf = vectorizer.fit_transform(df['summary'])\n",
        "tfidf = tfidf.toarray()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the vectorizer has built a vector that represents the summary field, \n",
        "built by using the number of times a word is present in a text, weighted by\n",
        "how many texts the word occurs in overall, in all the texts. \n",
        "We got a matrix with all our texts along the y-axis and the words along the x-axis:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf.shape)\n",
        "words = vectorizer.get_feature_names()\n",
        "print(len(words))\n",
        "print(words[10:20])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create a column in our dataframe with the vectors representing the summary text:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df['summary_vec'] = list(tfidf)\n",
        "\n",
        "print(df.head())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we want to do now is take X - all the texts (the summary column) \n",
        "in their vector representation - and y - the column we are using to \n",
        "predict them (The label column) - and split them into one training set and one test set.\n",
        "We'll use the first portion to train a classifier algorithm, and the \n",
        "second portion to test the classifier afterwards, to see how well it performed:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into test and train \n",
        "\n",
        "x = df['summary_vec'].tolist()\n",
        "y = df['Label'].tolist()\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,stratify=y, random_state=66)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to create and train the model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')\n",
        "model.fit(X=X_train, y=y_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets do the predictions on the test set, data that the classifier wasn't trained on:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X=X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "After predicting let's use some common measures for performance and test how well our model perform::"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "auc_weighted = roc_auc_score(y_test, y_pred,average=\"weighted\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(auc_weighted)\n",
        "print(accuracy)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We dont want to just run this locally, we would like to run this in a compute cluster in the cloud, and we want to be able to track metrics on how this training performed and so on, and make it available to others in our team"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create Experiment, my container for Runs\n",
        "\n",
        "from azureml.core import Experiment\n",
        "\n",
        "experiment = Experiment(workspace=ws, name=\"SecurityBugClassification\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create compute resource that I will be using for training my classifier\n",
        "# If a cluster by that name already exist, use it\n",
        "\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "import os\n",
        "\n",
        "\n",
        "# choose a name for your cluster\n",
        "compute_name = os.environ.get('AML_COMPUTE_CLUSTER_NAME', 'cpu-cluster')\n",
        "\n",
        "# I'll construct a cluster of nodes 0-1 because\n",
        "# I'll be working with scikit-learn and there's no scaling out to sev \n",
        "# nodes but I want the cluster to shut down when not in use\n",
        "compute_min_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MIN_NODES', 0)\n",
        "compute_max_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MAX_NODES', 1)\n",
        "\n",
        "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
        "vm_size = os.environ.get('AML_COMPUTE_CLUSTER_SKU', 'STANDARD_D2_V2')\n",
        "\n",
        "\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is AmlCompute:\n",
        "        print('found compute target. just use it. ' + compute_name)\n",
        "else:\n",
        "    print('creating a new compute target...')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
        "                                                                min_nodes=compute_min_nodes, \n",
        "                                                                max_nodes=compute_max_nodes)\n",
        "\n",
        "    # create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "    \n",
        "    # can poll for a minimum number of nodes and for a specific timeout. \n",
        "    # if no min node count is provided it will use the scale settings for the cluster\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "    \n",
        "     # For a more detailed view of current AmlCompute status, use get_status()\n",
        "    print(compute_target.get_status().serialize())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to submit a job to run on the remote training cluster we have created. To do that we need to:\n",
        "\n",
        "* Create a training script\n",
        "* Create an estimator object\n",
        "* Submit the job \n",
        "\n",
        "We will put the files that will be copied to the remote cluster nodes for execution in the folder \"train-dataset\":"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "script_folder = os.path.join(os.getcwd(), 'train-dataset')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The directory must contain a file with the training script you want to run. For better visibiilty into what the script does, we'll create the file here and add it to the directory we just created:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/train.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from azureml.core import Dataset, Run\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "\n",
        "run = Run.get_context()\n",
        "\n",
        "# get input dataset by name\n",
        "dataset = run.input_datasets['SecBugDataset']\n",
        "\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "\n",
        "# create column used as target\n",
        "\n",
        "df['Label'] = [1 if x =='Integrity/Security' else 0 for x in df['L2']]\n",
        "\n",
        "# do the vectorization - tf-idf\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=2)\n",
        "tfidf = vectorizer.fit_transform(df['summary'])\n",
        "tfidf = tfidf.toarray()\n",
        "\n",
        "# create our feature column\n",
        "df['summary_vec'] = list(tfidf)\n",
        "\n",
        "#dividing X,y into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['summary_vec'].tolist(), df['Label'].tolist(), test_size=0.2, random_state=66)\n",
        "\n",
        "\n",
        "# create our classifier & train it\n",
        "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')\n",
        "model.fit(X=X_train, y=y_train)\n",
        "\n",
        "# make predictions to see how well it does\n",
        "y_pred = model.predict(X=X_test)\n",
        "\n",
        "# measure it with to different metrics\n",
        "auc_weighted = roc_auc_score(y_test, y_pred,average=\"weighted\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# log the metrics we want to track and measure on to the Run\n",
        "run.log(\"AUC_Weighted\", auc_weighted)\n",
        "run.log(\"Accuracy\", accuracy)\n",
        "\n",
        "model_file_name = 'LogRegModel.pkl'\n",
        "\n",
        "\n",
        "# The training script saves the model into a directory named ‘outputs’. Files saved in the \n",
        "# outputs folder are automatically uploaded into experiment record. Anything written in this \n",
        "# directory is automatically uploaded into the workspace.\n",
        "os.makedirs('./outputs', exist_ok=True)\n",
        "with open(model_file_name, 'wb') as file:\n",
        "    joblib.dump(value=model, filename='outputs/' + model_file_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our training script we log important metrics to the current run, as well as saving the model created into a directory called 'outputs' that will be uploaded to the workspace and available through our run object when the training (run) is completed. Now, we need to create an estimator object that contains the run configuration:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.sklearn import SKLearn\n",
        "\n",
        "est = SKLearn(source_directory=script_folder, \n",
        "              entry_script='train.py', \n",
        "              inputs=[dataset.as_named_input('SecBugDataset')],\n",
        "              #environment_definition=env,\n",
        "              pip_packages=['azureml-dataprep[pandas]'],\n",
        "              compute_target=compute_target) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and we submit this to the Experiment it belongs to:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run = experiment.submit(config=est)\n",
        "run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run.wait_for_completion(show_output=True) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the contents of the output directory after the run:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(run.get_file_names())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets also register our model to the workspace so that we can retrieve it later for testing and deployment:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# register model \n",
        "model = run.register_model(model_name='LogRegModel.pkl', model_path='outputs/LogRegModel.pkl')\n",
        "print(model.name, model.id, model.version, sep='\\t')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, just running this model once, with no validation, no parameter tuning or testing out other algorithms to see if they perform better is not something we would to in reality - but for now, lets pretend we're satisfied and wants others to be able to use our model in a real world scenario. Then we need to deploy our model to a web service running in a container so that it can be consumed from other applications.\n",
        "\n",
        "For that we need:\n",
        "* A scoring script to show how to use the model\n",
        "* An environment file to show what packages need to be installed\n",
        "* A configuration file to build the ACI\n",
        "* The model we trained before\n",
        "\n",
        "Again, we will be creating the scoring script inline for visibility, called score.py. It is used by the web service call to show how to use the model.\n",
        "\n",
        "You must include two required functions into the scoring script:\n",
        "* The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started. \n",
        "\n",
        "* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "deploy_folder = os.path.join(os.getcwd(), 'deploy-model')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $deploy_folder/score.py\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from azureml.core.model import Model\n",
        "from azureml.core import model\n",
        "\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
        "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'LogRegModel.pkl')\n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "\n",
        "# note you can pass in multiple rows for scoring\n",
        "def run(raw_data):\n",
        "    try:\n",
        "        data = json.loads(raw_data)['data']\n",
        "        data = np.array(data)\n",
        "        result = model.predict([data])\n",
        "\n",
        "        # you can return any data type as long as it is JSON-serializable\n",
        "        return result.tolist()\n",
        "    except Exception as e:\n",
        "        result = str(e)\n",
        "        return result"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs `scikit-learn` and `azureml-sdk`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.conda_dependencies import CondaDependencies \n",
        "\n",
        "myenv = CondaDependencies()\n",
        "myenv.add_pip_package(\"scikit-learn==0.20.1\")\n",
        "myenv.add_pip_package(\"azureml-defaults\")\n",
        "myenv.add_pip_package('azureml-dataprep[pandas]')\n",
        "\n",
        "with open(\"./deploy-model/myenv.yml\",\"w\") as f:\n",
        "    f.write(myenv.serialize_to_string())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice import AciWebservice\n",
        "\n",
        "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
        "                                               memory_gb=1, \n",
        "                                               tags={\"data\": \"SecBugDataset\",  \"method\" : \"sklearn\"}, \n",
        "                                               description='Predict Security Bugs with sklearn')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the image and deploy. The following code goes through these steps:\n",
        "\n",
        "1. Create environment object containing dependencies needed by the model using the environment file (`myenv.yml`)\n",
        "1. Create inference configuration necessary to deploy the model as a web service using:\n",
        "   * The scoring file (`score.py`)\n",
        "   * environment object created in previous step\n",
        "1. Deploy the model to the ACI container.\n",
        "1. Get the web service HTTP endpoint."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from azureml.core.webservice import Webservice\n",
        "from azureml.core.model import InferenceConfig, Model\n",
        "from azureml.core.environment import Environment\n",
        "\n",
        "scorefile = os.path.join(os.getcwd(), 'deploy-model','score.py')\n",
        "myenvfile = os.path.join(os.getcwd(), 'deploy-model','myenv.yml')\n",
        "\n",
        "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=myenvfile)\n",
        "inference_config = InferenceConfig(entry_script=scorefile, environment=myenv)\n",
        "\n",
        "service = Model.deploy(workspace=ws, \n",
        "                       name='secbug-sklearn-logreg-svc-5', \n",
        "                       models=[model], \n",
        "                       inference_config=inference_config, \n",
        "                       deployment_config=aciconfig)\n",
        "\n",
        "service.wait_for_deployment(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(service.scoring_uri)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have come this far and are able to test our model locally in this notebook, but there are several things that we must deal with still. First, we would need to be able to call the scoring script with text input, not the vector. To accomplish this, we'd need to do some additional work - and also, it might seem like with the high accuracy score the model did well. But really, with a dataset where the amount of elements in the class we are trying to predict are so few, compared to the rest non-security items, high accuracy is not a good way to measure. The AUC is a better measure for this kind of skewed classes, and we got an auc score of 0.5, which isn't very good. It means the model cant really discriminate well between the two classes. Read more about that here: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 For now, we will move over to another notebook and another approach: Automated Machine Learning. "
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
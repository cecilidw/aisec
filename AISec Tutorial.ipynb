{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introtext\n",
    "\n",
    "To run the contents of the cell, press ctrl+enter :)\n",
    "\n",
    "\n",
    "The first thing we need to do is connect to the Azure Machine Learning Workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "ws=Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been uploaded and registeres in the workspace, so we just need to get it from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>L3</th>\n",
       "      <th>L4</th>\n",
       "      <th>L5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drag and drop of learning design zip file not ...</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Usability</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Capability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Error Launching Compendium LD after install</td>\n",
       "      <td>Installability</td>\n",
       "      <td>Reliability</td>\n",
       "      <td>Installability</td>\n",
       "      <td>Installability</td>\n",
       "      <td>Installability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiple column Arrange for map of unlinked no...</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Usability</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facility to assign node icon sets on a per map...</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Usability</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spell Checker</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary              L1  \\\n",
       "0  Drag and drop of learning design zip file not ...      Capability   \n",
       "1        Error Launching Compendium LD after install  Installability   \n",
       "2  Multiple column Arrange for map of unlinked no...    Requirements   \n",
       "3  Facility to assign node icon sets on a per map...    Requirements   \n",
       "4                                      Spell Checker    Requirements   \n",
       "\n",
       "             L2              L3              L4              L5  \n",
       "0     Usability      Capability      Capability      Capability  \n",
       "1   Reliability  Installability  Installability  Installability  \n",
       "2  Requirements       Usability    Requirements    Requirements  \n",
       "3  Requirements       Usability    Requirements    Requirements  \n",
       "4  Requirements    Requirements    Requirements    Requirements  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset from ws:\n",
    "dataset = Dataset.get_by_name(ws, name='SecBugDataset')\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "# this is the dataset we want to use:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the result from one of the labelers, found in the column L1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               summary                  L1  \\\n",
      "408  locked out of files due to forgotten user name...  Integrity/Security   \n",
      "575      Remove password length restriction (to zero?)  Integrity/Security   \n",
      "802        passwords less than five characters allowed  Integrity/Security   \n",
      "803   non-administrators have administrator privileges  Integrity/Security   \n",
      "804                        users can delete themselves  Integrity/Security   \n",
      "860                              Installation Problems  Integrity/Security   \n",
      "\n",
      "                     L2                  L3                  L4  \\\n",
      "408  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "575  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "802  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "803  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "804  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "860      Installability      Installability      Installability   \n",
      "\n",
      "                     L5  \n",
      "408  Integrity/Security  \n",
      "575  Integrity/Security  \n",
      "802  Integrity/Security  \n",
      "803  Integrity/Security  \n",
      "804  Integrity/Security  \n",
      "860      Installability  \n"
     ]
    }
   ],
   "source": [
    "# these are the bugs labeled as security bugs by labeler 1\n",
    "\n",
    "sb = df[df['L1'] == 'Integrity/Security']\n",
    "print(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# how many bugs are labeled as security bugs? \n",
    "\n",
    "print(len(sb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets run through locally what we want to deploy to run in the cloud:\n",
    "\n",
    "\n",
    "First of all, there are a couple of libraries we need to import. Internal AML libraries to work with datasets and to handle training runs that will be logged to our experiment, and libraries from the machine learning framework \"Scikit Learn\" which contains functionality for using a classifier algorithm to train on our dataset and output a model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could experiment with different labelers, but for now we will use the results from labeler 1, so we create a new column \"Label\", with the contents from the column with labels from labeler 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = [1 if x =='Integrity/Security' else 0 for x in df['L1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field we want to use to predict if it is a security bug or not is\n",
    "the \"summary\" field. It is a short text, and the text must be translated into a \n",
    "representation the machine learning alogrithm understand. For this we use the tf-idf vectorization algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the vectorization - tf-idf\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf = vectorizer.fit_transform(df['summary'])\n",
    "tfidf = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the vectorizer has built a vector that represents the summary field, \n",
    "built by using the number of times a word is present in a text, weighted by\n",
    "how many texts the word occurs in overall. \n",
    "We got a matrix with all our texts along the y-axis and the words along the x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(962, 727)\n",
      "727\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape)\n",
    "words = vectorizer.get_feature_names()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a column in our datafram with the vectors representing the summary text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             summary              L1  \\\n",
      "0  Drag and drop of learning design zip file not ...      Capability   \n",
      "1        Error Launching Compendium LD after install  Installability   \n",
      "2  Multiple column Arrange for map of unlinked no...    Requirements   \n",
      "3  Facility to assign node icon sets on a per map...    Requirements   \n",
      "4                                      Spell Checker    Requirements   \n",
      "\n",
      "             L2              L3              L4              L5  Label  \\\n",
      "0     Usability      Capability      Capability      Capability      0   \n",
      "1   Reliability  Installability  Installability  Installability      0   \n",
      "2  Requirements       Usability    Requirements    Requirements      0   \n",
      "3  Requirements       Usability    Requirements    Requirements      0   \n",
      "4  Requirements    Requirements    Requirements    Requirements      0   \n",
      "\n",
      "                                         summary_vec  \n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "df['summary_vec'] = list(tfidf)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do now is take X - all the texts (the summary column) \n",
    "in their vector representation - and y - the column we are using to \n",
    "predict them - and split them into one training set and one test set.\n",
    "We'll use the first portion to train a classifier algorithm, and the \n",
    "second portion to test the classifier afterwards, to see how well it performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into test and train \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['summary_vec'].tolist(), df['Label'].tolist(), test_size=0.2, random_state=66)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')\n",
    "model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the predictions on the test set, data that the classifier wasn't trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After predicting let's use some common measures for performance and test how well our model perform::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.9948186528497409\n"
     ]
    }
   ],
   "source": [
    "auc_weighted = roc_auc_score(y_test, y_pred,average=\"weighted\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(auc_weighted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont want to just run this locally, we would like to run this in a compute cluster in the cloud, and we want to be able to track metrics on how this training performed and so on, and make it available to others in our team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Experiment, my container for Runs\n",
    "\n",
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(workspace=ws, name=\"SecurityBugClassification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "# create compute resource that I will be using for training my classifier\n",
    "# If a cluster by that name already exist, use it\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get('AML_COMPUTE_CLUSTER_NAME', 'cpu-cluster')\n",
    "\n",
    "# I'll construct a cluster of nodes 0-1 because\n",
    "# I'll be working with scikit-learn and there's no scaling out to sev \n",
    "# nodes but I want the cluster to shut down when not in use\n",
    "compute_min_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MIN_NODES', 0)\n",
    "compute_max_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MAX_NODES', 1)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get('AML_COMPUTE_CLUSTER_SKU', 'STANDARD_D2_V2')\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes, \n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to submit a job to run on the remote training cluster we have created. To do that we need to:\n",
    "\n",
    "* Create a training script\n",
    "* Create an estimator object\n",
    "* Submit the job \n",
    "\n",
    "We will put the files that will be copied to the remote cluster nodes for execution in the folder \"train-dataset\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = os.path.join(os.getcwd(), 'train-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory must contain a file with the training script you want to run. For better visibiilty into what the script does, we'll create the file here and add it to the directory we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\Users\\cewidste\\source\\repos\\cecilidw\\aisec\\train-dataset/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# get input dataset by name\n",
    "dataset = run.input_datasets['SecBugDataset']\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "# create column used as target, experiment with different labelers\n",
    "\n",
    "df['Label'] = [1 if x =='Integrity/Security' else 0 for x in df['L1']]\n",
    "\n",
    "# do the vectorization - tf-idf\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf = vectorizer.fit_transform(df['summary'])\n",
    "tfidf = tfidf.toarray()\n",
    "\n",
    "# create our feature column\n",
    "df['summary_vec'] = list(tfidf)\n",
    "\n",
    "#dividing X,y into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['summary_vec'].tolist(), df['Label'].tolist(), test_size=0.2, random_state=66)\n",
    "\n",
    "\n",
    "# create our classifier & train it\n",
    "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')\n",
    "model.fit(X=X_train, y=y_train)\n",
    "\n",
    "# make predictions to see how well it does\n",
    "y_pred = model.predict(X=X_test)\n",
    "\n",
    "# measure it with to different metrics\n",
    "auc_weighted = roc_auc_score(y_test, y_pred,average=\"weighted\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# log the metrics we want to track and measure on to the Run\n",
    "run.log(\"AUC_Weighted\", auc_weighted)\n",
    "run.log(\"Accuracy\", accuracy)\n",
    "\n",
    "model_file_name = 'LogRegModel.pkl'\n",
    "\n",
    "\n",
    "# The training script saves the model into a directory named ‘outputs’. Files saved in the \n",
    "# outputs folder are automatically uploaded into experiment record. Anything written in this \n",
    "# directory is automatically uploaded into the workspace.\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "with open(model_file_name, 'wb') as file:\n",
    "    joblib.dump(value=model, filename='outputs/' + model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training script we log important metrics to the current run, as well as saving the model created into a directory called 'outputs' that will be uploaded to the workspace and available through our run object when the training (run) is completed. Now, we need to create an estimator object that contains the run configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "est = SKLearn(source_directory=script_folder, \n",
    "              entry_script='train.py', \n",
    "              inputs=[dataset.as_named_input('SecBugDataset')],\n",
    "              #environment_definition=env,\n",
    "              pip_packages=['azureml-dataprep[pandas]'],\n",
    "              compute_target=compute_target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we submit this to the Experiment it belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>SecurityBugClassification</td><td>SecurityBugClassification_1588943785_b4bb4009</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/SecurityBugClassification/runs/SecurityBugClassification_1588943785_b4bb4009?wsid=/subscriptions/7f150ec6-cc4b-4575-b242-1d8de759c3ab/resourcegroups/cdwaisec/workspaces/cdwaisecws\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: SecurityBugClassification,\n",
       "Id: SecurityBugClassification_1588943785_b4bb4009,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the contents of the output directory after the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also register our model to the workspace so that we can retrieve it later for testing and deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register model \n",
    "model = run.register_model(model_name='LogRegModel.pkl', model_path='outputs/LogRegModel.pkl')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just running this model once, with no validation, no parameter tuning or testing out other algorithms to see if they perform better is not something we would to in reality - but for now, lets pretend we're satisfied and wants others to be able to use our model in a real world scenario. Then we need to deploy our model to a web service running in a container so that it can be consumed from other applications.\n",
    "\n",
    "For that we need:\n",
    "* A scoring script to show how to use the model\n",
    "* An environment file to show what packages need to be installed\n",
    "* A configuration file to build the ACI\n",
    "* The model we trained before\n",
    "\n",
    "Again, we will be creating the scoring script inline for visibility, called score.py. It is used by the web service call to show how to use the model.\n",
    "\n",
    "You must include two required functions into the scoring script:\n",
    "* The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started. \n",
    "\n",
    "* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_folder = os.path.join(os.getcwd(), 'deploy-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\Users\\cewidste\\source\\repos\\cecilidw\\aisec\\deploy-model/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $deploy_folder/score.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import model\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'LogRegModel.pkl')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = np.array(data)\n",
    "        result = model.predict(data)\n",
    "\n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs `scikit-learn` and `azureml-sdk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_pip_package(\"scikit-learn==0.20.1\")\n",
    "myenv.add_pip_package(\"azureml-defaults\")\n",
    "myenv.add_pip_package('azureml-dataprep[pandas]')\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"tertre\",  \"method\" : \"sklearn\"}, \n",
    "                                               description='Predict Security Bugs with sklearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "1. Create environment object containing dependencies needed by the model using the environment file (`myenv.yml`)\n",
    "1. Create inference configuration necessary to deploy the model as a web service using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * environment object created in previous step\n",
    "1. Deploy the model to the ACI container.\n",
    "1. Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code to write to deploy_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name='secbug-sklearn-logreg-svc', \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'service' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-03d738627f22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscoring_uri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'service' is not defined"
     ]
    }
   ],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the deployed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "service = ws.webservices['secbug-sklearn-logreg-svc']\n",
    "\n",
    "# scrape the first row from the test set.\n",
    "test_samples = json.dumps({\"data\": X_test[0].tolist()})\n",
    "\n",
    "\n",
    "#score on our service\n",
    "service.run(input_data = test_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cdwaisec)",
   "language": "python",
   "name": "cdwaisec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

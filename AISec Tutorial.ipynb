{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start up trying to implement an end-to-end process with similar algorithm and preprocessing as described in the original article (https://docs.microsoft.com/en-us/security/engineering/identifying-security-bug-reports) and see where that gets us first. \n",
    "\n",
    "If you are new to Jupyter Notebooks, find instructions on use here: \n",
    "To run the contents of a cell, press ctrl+enter :)\n",
    "\n",
    "The first thing we need to do is connect to the Azure Machine Learning Workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "ws=Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been uploaded and registered in the workspace already, so we just need to get it from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>L3</th>\n",
       "      <th>L4</th>\n",
       "      <th>L5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drag and drop of learning design zip file not ...</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Usability</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Capability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Error Launching Compendium LD after install</td>\n",
       "      <td>Installability</td>\n",
       "      <td>Reliability</td>\n",
       "      <td>Installability</td>\n",
       "      <td>Installability</td>\n",
       "      <td>Installability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiple column Arrange for map of unlinked no...</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Usability</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facility to assign node icon sets on a per map...</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Usability</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spell Checker</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Requirements</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary              L1  \\\n",
       "0  Drag and drop of learning design zip file not ...      Capability   \n",
       "1        Error Launching Compendium LD after install  Installability   \n",
       "2  Multiple column Arrange for map of unlinked no...    Requirements   \n",
       "3  Facility to assign node icon sets on a per map...    Requirements   \n",
       "4                                      Spell Checker    Requirements   \n",
       "\n",
       "             L2              L3              L4              L5  \n",
       "0     Usability      Capability      Capability      Capability  \n",
       "1   Reliability  Installability  Installability  Installability  \n",
       "2  Requirements       Usability    Requirements    Requirements  \n",
       "3  Requirements       Usability    Requirements    Requirements  \n",
       "4  Requirements    Requirements    Requirements    Requirements  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset from ws:\n",
    "dataset = Dataset.get_by_name(ws, name='SecBugDataset')\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "# this is the dataset we want to use:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the result from one of the labelers, found in the column L2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               summary                  L1  \\\n",
      "204                 No Passwords (may be resolved now)        Requirements   \n",
      "408  locked out of files due to forgotten user name...  Integrity/Security   \n",
      "471                  bad file permissions  as packaged      Installability   \n",
      "575      Remove password length restriction (to zero?)  Integrity/Security   \n",
      "680         Either password or user name is not stored         Reliability   \n",
      "758   You should not be able to create a duplicate tag           Usability   \n",
      "802        passwords less than five characters allowed  Integrity/Security   \n",
      "803   non-administrators have administrator privileges  Integrity/Security   \n",
      "804                        users can delete themselves  Integrity/Security   \n",
      "838  Readers feature lists one user under another u...         Reliability   \n",
      "869                   All data in map node disappeared        Requirements   \n",
      "880  Node background color gets lost during backup ...           Usability   \n",
      "893                  Add timespans to backup / restore        Requirements   \n",
      "922           Site \"compendiuminstitute.org\" defaced ?       Documentation   \n",
      "\n",
      "                     L2                  L3                  L4  \\\n",
      "204  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "408  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "471  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "575  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "680  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "758  Integrity/Security        Requirements           Usability   \n",
      "802  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "803  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "804  Integrity/Security  Integrity/Security  Integrity/Security   \n",
      "838  Integrity/Security           Usability  Integrity/Security   \n",
      "869  Integrity/Security           Usability         Reliability   \n",
      "880  Integrity/Security           Usability           Usability   \n",
      "893  Integrity/Security           Usability        Requirements   \n",
      "922  Integrity/Security           Usability       Documentation   \n",
      "\n",
      "                     L5  \n",
      "204        Requirements  \n",
      "408  Integrity/Security  \n",
      "471  Integrity/Security  \n",
      "575  Integrity/Security  \n",
      "680  Integrity/Security  \n",
      "758           Usability  \n",
      "802  Integrity/Security  \n",
      "803  Integrity/Security  \n",
      "804  Integrity/Security  \n",
      "838  Integrity/Security  \n",
      "869         Reliability  \n",
      "880           Usability  \n",
      "893        Requirements  \n",
      "922       Documentation  \n"
     ]
    }
   ],
   "source": [
    "# these are the bugs labeled as security bugs by labeler 2\n",
    "\n",
    "sb = df[df['L2'] == 'Integrity/Security']\n",
    "print(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# how many bugs are labeled as security bugs? \n",
    "\n",
    "print(len(sb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets run through locally what we want to deploy to run in the cloud:\n",
    "\n",
    "\n",
    "First of all, there are a couple of libraries we need to import. Internal AML libraries to work with datasets and to handle training runs that will be logged to our experiment, and libraries from the machine learning framework \"Scikit Learn\" which contains functionality for using a classifier algorithm to train on our dataset and output a model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could experiment with different labelers or even combine the results from the labelers as in the original article this dataset was retrieved from, but for now we will use the results from labeler 2, so we create a new column \"Label\", with the contents from the column with labels from labeler 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = [1 if x =='Integrity/Security' else 0 for x in df['L2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field we want to use to predict if it is a security bug or not is\n",
    "the \"summary\" field. It is a short text, and the text must be translated into a \n",
    "representation the machine learning alogrithm understand. For this we use the tf-idf vectorization algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the vectorization - tf-idf\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf = vectorizer.fit_transform(df['summary'])\n",
    "tfidf = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the vectorizer has built a vector that represents the summary field, \n",
    "built by using the number of times a word is present in a text, weighted by\n",
    "how many texts the word occurs in overall, in all the texts. \n",
    "We got a matrix with all our texts along the y-axis and the words along the x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(962, 727)\n",
      "727\n",
      "['action', 'actions', 'activity', 'actual', 'add', 'adding', 'adds', 'aerial', 'after', 'against']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape)\n",
    "words = vectorizer.get_feature_names()\n",
    "print(len(words))\n",
    "print(words[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a column in our dataframe with the vectors representing the summary text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             summary              L1  \\\n",
      "0  Drag and drop of learning design zip file not ...      Capability   \n",
      "1        Error Launching Compendium LD after install  Installability   \n",
      "2  Multiple column Arrange for map of unlinked no...    Requirements   \n",
      "3  Facility to assign node icon sets on a per map...    Requirements   \n",
      "4                                      Spell Checker    Requirements   \n",
      "\n",
      "             L2              L3              L4              L5  Label  \\\n",
      "0     Usability      Capability      Capability      Capability      0   \n",
      "1   Reliability  Installability  Installability  Installability      0   \n",
      "2  Requirements       Usability    Requirements    Requirements      0   \n",
      "3  Requirements       Usability    Requirements    Requirements      0   \n",
      "4  Requirements    Requirements    Requirements    Requirements      0   \n",
      "\n",
      "                                         summary_vec  \n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "df['summary_vec'] = list(tfidf)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do now is take X - all the texts (the summary column) \n",
    "in their vector representation - and y - the column we are using to \n",
    "predict them (The label column) - and split them into one training set and one test set.\n",
    "We'll use the first portion to train a classifier algorithm, and the \n",
    "second portion to test the classifier afterwards, to see how well it performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into test and train \n",
    "\n",
    "x = df['summary_vec'].tolist()\n",
    "y = df['Label'].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,stratify=y, random_state=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')\n",
    "model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the predictions on the test set, data that the classifier wasn't trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After predicting let's use some common measures for performance and test how well our model perform::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.9844559585492227\n"
     ]
    }
   ],
   "source": [
    "auc_weighted = roc_auc_score(y_test, y_pred,average=\"weighted\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(auc_weighted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont want to just run this locally, we would like to run this in a compute cluster in the cloud, and we want to be able to track metrics on how this training performed and so on, and make it available to others in our team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Experiment, my container for Runs\n",
    "\n",
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(workspace=ws, name=\"SecurityBugClassification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "# create compute resource that I will be using for training my classifier\n",
    "# If a cluster by that name already exist, use it\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get('AML_COMPUTE_CLUSTER_NAME', 'cpu-cluster')\n",
    "\n",
    "# I'll construct a cluster of nodes 0-1 because\n",
    "# I'll be working with scikit-learn and there's no scaling out to sev \n",
    "# nodes but I want the cluster to shut down when not in use\n",
    "compute_min_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MIN_NODES', 0)\n",
    "compute_max_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MAX_NODES', 1)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get('AML_COMPUTE_CLUSTER_SKU', 'STANDARD_D2_V2')\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes, \n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to submit a job to run on the remote training cluster we have created. To do that we need to:\n",
    "\n",
    "* Create a training script\n",
    "* Create an estimator object\n",
    "* Submit the job \n",
    "\n",
    "We will put the files that will be copied to the remote cluster nodes for execution in the folder \"train-dataset\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = os.path.join(os.getcwd(), 'train-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory must contain a file with the training script you want to run. For better visibiilty into what the script does, we'll create the file here and add it to the directory we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\Users\\cewidste\\source\\repos\\cecilidw\\aisec\\train-dataset/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# get input dataset by name\n",
    "dataset = run.input_datasets['SecBugDataset']\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "# create column used as target\n",
    "\n",
    "df['Label'] = [1 if x =='Integrity/Security' else 0 for x in df['L2']]\n",
    "\n",
    "# do the vectorization - tf-idf\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf = vectorizer.fit_transform(df['summary'])\n",
    "tfidf = tfidf.toarray()\n",
    "\n",
    "# create our feature column\n",
    "df['summary_vec'] = list(tfidf)\n",
    "\n",
    "#dividing X,y into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['summary_vec'].tolist(), df['Label'].tolist(), test_size=0.2, random_state=66)\n",
    "\n",
    "\n",
    "# create our classifier & train it\n",
    "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')\n",
    "model.fit(X=X_train, y=y_train)\n",
    "\n",
    "# make predictions to see how well it does\n",
    "y_pred = model.predict(X=X_test)\n",
    "\n",
    "# measure it with to different metrics\n",
    "auc_weighted = roc_auc_score(y_test, y_pred,average=\"weighted\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# log the metrics we want to track and measure on to the Run\n",
    "run.log(\"AUC_Weighted\", auc_weighted)\n",
    "run.log(\"Accuracy\", accuracy)\n",
    "\n",
    "model_file_name = 'LogRegModel.pkl'\n",
    "\n",
    "\n",
    "# The training script saves the model into a directory named ‘outputs’. Files saved in the \n",
    "# outputs folder are automatically uploaded into experiment record. Anything written in this \n",
    "# directory is automatically uploaded into the workspace.\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "with open(model_file_name, 'wb') as file:\n",
    "    joblib.dump(value=model, filename='outputs/' + model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training script we log important metrics to the current run, as well as saving the model created into a directory called 'outputs' that will be uploaded to the workspace and available through our run object when the training (run) is completed. Now, we need to create an estimator object that contains the run configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "est = SKLearn(source_directory=script_folder, \n",
    "              entry_script='train.py', \n",
    "              inputs=[dataset.as_named_input('SecBugDataset')],\n",
    "              #environment_definition=env,\n",
    "              pip_packages=['azureml-dataprep[pandas]'],\n",
    "              compute_target=compute_target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we submit this to the Experiment it belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>SecurityBugClassification</td><td>SecurityBugClassification_1589962080_d52c2b7c</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/SecurityBugClassification/runs/SecurityBugClassification_1589962080_d52c2b7c?wsid=/subscriptions/7f150ec6-cc4b-4575-b242-1d8de759c3ab/resourcegroups/cdwaisec/workspaces/cdwaisecws\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: SecurityBugClassification,\n",
       "Id: SecurityBugClassification_1589962080_d52c2b7c,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: SecurityBugClassification_1589962080_d52c2b7c\n",
      "Web View: https://ml.azure.com/experiments/SecurityBugClassification/runs/SecurityBugClassification_1589962080_d52c2b7c?wsid=/subscriptions/7f150ec6-cc4b-4575-b242-1d8de759c3ab/resourcegroups/cdwaisec/workspaces/cdwaisecws\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2020-05-20T08:11:42Z Starting output-watcher...\n",
      "2020-05-20T08:11:42Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "Entering job preparation. Current time:2020-05-20T08:13:42.939383\n",
      "Starting job preparation. Current time:2020-05-20T08:13:43.939675\n",
      "Extracting the control code.\n",
      "fetching and extracting the control code on master node.\n",
      "Retrieving project from snapshot: 0bda5a85-206d-48e8-8f06-9c97bcf72df1\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 49\n",
      "Starting project file download.\n",
      "Finished project file download.\n",
      "downloadDataStore - Download from datastores if requested.\n",
      "Entering context manager injector. Current time:2020-05-20T08:13:45.616933\n",
      "downloadDataStore completed\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "2020/05/20 08:13:46 Instrumentation Key Is Empty Skipping App Insight Logger\n",
      "Entering context manager injector. Current time:2020-05-20T08:13:48.467955\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 99\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ train.py ] with arguments: []\n",
      "After variable expansion, calling script [ train.py ] with arguments: []\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 99\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.7351129055023193 seconds\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "Entering job release. Current time:2020-05-20T08:14:12.714067\n",
      "Starting job release. Current time:2020-05-20T08:14:13.765210\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 288\n",
      "Entering context manager injector. Current time:2020-05-20T08:14:13.790684\n",
      "Job release is complete. Current time:2020-05-20T08:14:15.525726\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: SecurityBugClassification_1589962080_d52c2b7c\n",
      "Web View: https://ml.azure.com/experiments/SecurityBugClassification/runs/SecurityBugClassification_1589962080_d52c2b7c?wsid=/subscriptions/7f150ec6-cc4b-4575-b242-1d8de759c3ab/resourcegroups/cdwaisec/workspaces/cdwaisecws\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'SecurityBugClassification_1589962080_d52c2b7c',\n",
       " 'target': 'cpu-cluster',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-05-20T08:11:39.400671Z',\n",
       " 'endTimeUtc': '2020-05-20T08:14:20.617611Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': '46b65f16-52f7-4cc5-b30d-c1e90386c414',\n",
       "  'azureml.git.repository_uri': 'https://github.com/cecilidw/aisec.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/cecilidw/aisec.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '38acc35452c478b56559211831ee44591cbcf8b9',\n",
       "  'mlflow.source.git.commit': '38acc35452c478b56559211831ee44591cbcf8b9',\n",
       "  'azureml.git.dirty': 'False',\n",
       "  'AzureML.DerivedImageName': 'azureml/azureml_1999fca195421d007aa0488b61b21d4a',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': '5346d049-df84-45b3-85b1-54ed5b4319df'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'SecBugDataset', 'mechanism': 'Direct'}}],\n",
       " 'runDefinition': {'script': 'train.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': [],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'cpu-cluster',\n",
       "  'dataReferences': {},\n",
       "  'data': {'SecBugDataset': {'dataLocation': {'dataset': {'id': '5346d049-df84-45b3-85b1-54ed5b4319df'},\n",
       "     'dataPath': None},\n",
       "    'createOutputDirectories': False,\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'SecBugDataset',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment SecurityBugClassification Environment',\n",
       "   'version': 'Autosave_2020-05-07T13:55:40Z_eb4ffa2a',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-dataprep[pandas]',\n",
       "        'azureml-defaults',\n",
       "        'scikit-learn==0.20.3',\n",
       "        'scipy==1.2.1',\n",
       "        'numpy==1.16.2',\n",
       "        'joblib==0.13.2']}],\n",
       "     'name': 'azureml_3fc31fe37ae4feefa22d0e0765e1084b'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt': 'https://cdwaisecws0163584650.blob.core.windows.net/azureml/ExperimentRun/dcid.SecurityBugClassification_1589962080_d52c2b7c/azureml-logs/55_azureml-execution-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt?sv=2019-02-02&sr=b&sig=7dLlles5kwe5016GL%2B6E%2Bl1YkO4pFaYfjVYe9jAnT%2Bc%3D&st=2020-05-20T08%3A04%3A21Z&se=2020-05-20T16%3A14%3A21Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt': 'https://cdwaisecws0163584650.blob.core.windows.net/azureml/ExperimentRun/dcid.SecurityBugClassification_1589962080_d52c2b7c/azureml-logs/65_job_prep-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt?sv=2019-02-02&sr=b&sig=v5eKjc%2BtA6FBNgN7oyxkovJ4EVGjNjiRgfWb0hBZXzI%3D&st=2020-05-20T08%3A04%3A21Z&se=2020-05-20T16%3A14%3A21Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://cdwaisecws0163584650.blob.core.windows.net/azureml/ExperimentRun/dcid.SecurityBugClassification_1589962080_d52c2b7c/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=auzzhY0oZaLHCzb1ucJ44Ult8TTsx21pnd%2BNWtmJuzg%3D&st=2020-05-20T08%3A04%3A21Z&se=2020-05-20T16%3A14%3A21Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt': 'https://cdwaisecws0163584650.blob.core.windows.net/azureml/ExperimentRun/dcid.SecurityBugClassification_1589962080_d52c2b7c/azureml-logs/75_job_post-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt?sv=2019-02-02&sr=b&sig=%2BYuWMa97ilrVmnSfmlyB27ZG0jOf%2BQmyalSsHRf9YHk%3D&st=2020-05-20T08%3A04%3A21Z&se=2020-05-20T16%3A14%3A21Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://cdwaisecws0163584650.blob.core.windows.net/azureml/ExperimentRun/dcid.SecurityBugClassification_1589962080_d52c2b7c/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=zrfFHRAyJ3bMIYCSPy26xscKn6NUPTVm2kwFAwV4kAQ%3D&st=2020-05-20T08%3A04%3A21Z&se=2020-05-20T16%3A14%3A21Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://cdwaisecws0163584650.blob.core.windows.net/azureml/ExperimentRun/dcid.SecurityBugClassification_1589962080_d52c2b7c/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=dphx5lZuyD6YltlYs4Hlo8WSyPvN2fdSUptgP2qbO9Y%3D&st=2020-05-20T08%3A04%3A21Z&se=2020-05-20T16%3A14%3A21Z&sp=r',\n",
       "  'logs/azureml/99_azureml.log': 'https://cdwaisecws0163584650.blob.core.windows.net/azureml/ExperimentRun/dcid.SecurityBugClassification_1589962080_d52c2b7c/logs/azureml/99_azureml.log?sv=2019-02-02&sr=b&sig=SzsZsQ6ukADbQr7VQ4t5dG3iV8t%2BrxCCBHoKBuPxdPo%3D&st=2020-05-20T08%3A04%3A21Z&se=2020-05-20T16%3A14%3A21Z&sp=r',\n",
       "  'logs/azureml/job_prep_azureml.log': 'https://cdwaisecws0163584650.blob.core.windows.net/azureml/ExperimentRun/dcid.SecurityBugClassification_1589962080_d52c2b7c/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=xCofVWvBb2eW7PQAqlFgNbalrl3U8Waq0F5i24Jcbsk%3D&st=2020-05-20T08%3A04%3A21Z&se=2020-05-20T16%3A14%3A21Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://cdwaisecws0163584650.blob.core.windows.net/azureml/ExperimentRun/dcid.SecurityBugClassification_1589962080_d52c2b7c/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=YrOQjyvIOzm0QzaEZxfXgdvdgTb%2FsbG8g2OrIx%2FLNuE%3D&st=2020-05-20T08%3A04%3A21Z&se=2020-05-20T16%3A14%3A21Z&sp=r'}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the contents of the output directory after the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/55_azureml-execution-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt', 'azureml-logs/65_job_prep-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_0b7fdd7b853416f97e7562a48e37e8bfac2482ddab534a7f2bdfebbba6f42506_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/99_azureml.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log', 'outputs/LogRegModel.pkl']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also register our model to the workspace so that we can retrieve it later for testing and deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegModel.pkl\tLogRegModel.pkl:4\t4\n"
     ]
    }
   ],
   "source": [
    "# register model \n",
    "model = run.register_model(model_name='LogRegModel.pkl', model_path='outputs/LogRegModel.pkl')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just running this model once, with no validation, no parameter tuning or testing out other algorithms to see if they perform better is not something we would to in reality - but for now, lets pretend we're satisfied and wants others to be able to use our model in a real world scenario. Then we need to deploy our model to a web service running in a container so that it can be consumed from other applications.\n",
    "\n",
    "For that we need:\n",
    "* A scoring script to show how to use the model\n",
    "* An environment file to show what packages need to be installed\n",
    "* A configuration file to build the ACI\n",
    "* The model we trained before\n",
    "\n",
    "Again, we will be creating the scoring script inline for visibility, called score.py. It is used by the web service call to show how to use the model.\n",
    "\n",
    "You must include two required functions into the scoring script:\n",
    "* The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started. \n",
    "\n",
    "* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_folder = os.path.join(os.getcwd(), 'deploy-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\Users\\cewidste\\source\\repos\\cecilidw\\aisec\\deploy-model/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $deploy_folder/score.py\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import model\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'LogRegModel.pkl')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = np.array(data)\n",
    "        result = model.predict([data])\n",
    "\n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs `scikit-learn` and `azureml-sdk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_pip_package(\"scikit-learn==0.20.1\")\n",
    "myenv.add_pip_package(\"azureml-defaults\")\n",
    "myenv.add_pip_package('azureml-dataprep[pandas]')\n",
    "\n",
    "with open(\"./deploy-model/myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"SecBugDataset\",  \"method\" : \"sklearn\"}, \n",
    "                                               description='Predict Security Bugs with sklearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "1. Create environment object containing dependencies needed by the model using the environment file (`myenv.yml`)\n",
    "1. Create inference configuration necessary to deploy the model as a web service using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * environment object created in previous step\n",
    "1. Deploy the model to the ACI container.\n",
    "1. Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...............................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "scorefile = os.path.join(os.getcwd(), 'deploy-model','score.py')\n",
    "myenvfile = os.path.join(os.getcwd(), 'deploy-model','myenv.yml')\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=myenvfile)\n",
    "inference_config = InferenceConfig(entry_script=scorefile, environment=myenv)\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name='secbug-sklearn-logreg-svc-5', \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://9f63b9fa-3263-49ce-977d-1fb4a39ebc1c.westeurope.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come this far and are able to test our model locally in this notebook, but there are several things that we must deal with still. First, we would need to be able to call the scoring script with text input, not the vector. To accomplish this, we'd need to do some additional work - and also, it might seem like with the high accuracy score the model did well. But really, with a dataset where the amount of elements in the class we are trying to predict are so few, compared to the rest non-security items, high accuracy is not a good way to measure. The AUC is a better measure for this kind of skewed classes, and we got an auc score of 0.5, which isn't very good. It means the model cant really discriminate well between the two classes. Read more about that here: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 For now, we will move over to another notebook and another approach: Automated Machine Learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cdwaisec)",
   "language": "python",
   "name": "cdwaisec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not an expert data scientist with a history of many years of experience in tuning different algorithms to different types of tasks in the ML domain, or if you are one, but just want to generate a baseline of testing different models and parameters - AutoML to the rescue! In this notebook we will use the automated machine learning capabilities in Azure Machine Learning to create a model that performs best given our dataset, our problem and what we want to predict.\n",
    "\n",
    "More about the AutoML capabilities in Azure Machine Learning here: https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml\n",
    "\n",
    "If you are new to Jupyter Notebooks, find instructions on use here: https://jupyter-notebook.readthedocs.io/en/stable/ \n",
    "\n",
    "Short version: To run the contents of a cell, press ctrl+enter :)\n",
    "\n",
    "\n",
    "We'll start with importing some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the Azure Machine Learning Workspace & the SecurityBugClassification experiment that has been created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# automl_folder = './automl_output/'\n",
    "automl_folder = os.path.join(os.getcwd(), 'automl_output')\n",
    "exp_name = 'SecurityBugClassification'\n",
    "\n",
    "experiment = Experiment(ws, exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been uploaded and registered in the workspace, so we just need to get it from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset from ws:\n",
    "dataset = Dataset.get_by_name(ws, name='SecBugDatasetLabelL2')\n",
    "\n",
    "X = dataset.keep_columns(columns=['summary'])\n",
    "y = dataset.keep_columns(columns=['Label L2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are again using the result from one of the labelers, found in the column L2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.take(5).to_pandas_dataframe())\n",
    "print(y.take(5).to_pandas_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create compute resource that I will be using for running automl\n",
    "# If a cluster by that name already exist, use it\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get('AML_COMPUTE_CLUSTER_NAME', 'cpu-cluster4n')\n",
    "\n",
    "# I'll construct a cluster of nodes 0-4 because\n",
    "# I want the cluster to shut down when not in use - if min nodes is 0 it autoshutdown when not in use\n",
    "compute_min_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MIN_NODES', 0)\n",
    "compute_max_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MAX_NODES', 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get('AML_COMPUTE_CLUSTER_SKU', 'STANDARD_D2_V2')\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes, \n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll configure a Conda environment for the AutoML job to use during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "import pkg_resources\n",
    "\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "conda_run_config.target = compute_target\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "\n",
    "cd = CondaDependencies.create(conda_packages=['numpy','scikit-learn','py-xgboost<=0.80'],\n",
    "                              pip_packages=['azureml-train-automl'])\n",
    "\n",
    "conda_run_config.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoML configuration will define various settings that are used, like how many iterations we want it to test (num of algorithm and parameter combinations to test) and what kind of metric to measure on. We'll also tell it to preprocess the data which will take care of the vectorization of the text. We'll tell it to use 5-fold cross validation since we dont have that much data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 5,\n",
    "    \"iterations\": 20,\n",
    "    \"n_cross_validations\": 5,\n",
    "    \"primary_metric\": 'AUC_weighted',\n",
    "    \"preprocess\": True,\n",
    "    \"max_concurrent_iterations\": 3,\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"verbosity\": logging.INFO\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             path = automl_folder,\n",
    "                             run_configuration=conda_run_config,\n",
    "                             X = X,\n",
    "                             y = y,\n",
    "                             **automl_settings)\n",
    "\n",
    "remote_run = experiment.submit(automl_config, show_output = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a widget to watch the progress of the run in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Automl job is finished we will have a bunch of different runs with trained models including preprocessing. We can inspect these in the portal, and likely we want to deploy the one with the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "print(\"Run:\", best_run)\n",
    "print(\"Model:\", fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a quick check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(['The formatting of the form is skewed','password and username can be leaked', 'administrators have way too many permissions'], columns = ['summary'])\n",
    "fitted_model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, we want to register this one and deploy it to ACI and make a REST endpoint available so that it can be called from another application that needs to classify security bugs based on the title!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'AutoML Model for Security Bug Classification'\n",
    "tags = {\"dataset\": \"SecBugDatasetLabelL2\"}\n",
    "model = remote_run.register_model(description = description, tags = tags)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, for deployment all we need is:\n",
    "\n",
    "* A scoring script to show how to use the model\n",
    "* An environment file to show what packages need to be installed\n",
    "* A configuration file to build the ACI\n",
    "* The model we trained before\n",
    "\n",
    "Important to notice: you will have to switch the value of the model_name parameter below with results from your own run in the output from the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./deploy-model/automlscore.py\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import azureml.train.automl\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path(model_name = \"AutoMLcaffbf1d018\") #model_name should be the output parameter \"id\" from the previous cell, up until the colon\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(rawdata):\n",
    "    try:\n",
    "        data = json.loads(rawdata)['text']\n",
    "        result = model.predict(pd.DataFrame(data, columns = ['summary']))\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})\n",
    "    return json.dumps({\"result\": result.tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an environment file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env_file_name = 'automl-secbugclassification-env.yml'\n",
    "cd.save_to_file('./deploy-model/', conda_env_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a deployment configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"SecBugDataset\",  \"method\" : \"automl\"}, \n",
    "                                               description='Predict Security Bugs with automl model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the image and deploy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "scorefile = os.path.join(os.getcwd(), 'deploy-model','automlscore.py')\n",
    "myenvfile = os.path.join(os.getcwd(), 'deploy-model','automl-secbugclassification-env.yml')\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=myenvfile)\n",
    "inference_config = InferenceConfig(entry_script=scorefile, environment=myenv)\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name='secbug-automl-svc-7', \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the deployed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "data = {\"text\": ['The formatting of the form is skewed','password and username can be leaked', 'administrators have way too many permissions']}\n",
    "\n",
    "test_samples = json.dumps(data)\n",
    "print(test_samples)\n",
    "\n",
    "resp = requests.post(service.scoring_uri, json=data, headers=headers)\n",
    "print(\"Prediction Results:\", resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bug Titles to score:\", test_samples)\n",
    "\n",
    "resp = requests.post(service.scoring_uri, json=data, headers=headers)\n",
    "print(\"Prediction Results:\", resp.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cdwaisec)",
   "language": "python",
   "name": "cdwaisec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
